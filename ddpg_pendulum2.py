import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import time
from collections import deque
import matplotlib.pyplot as plt # å¼•å…¥ç»˜å›¾åº“
# vvvvvvvvvvvvvv  åœ¨è¿™é‡Œæ·»åŠ ä¸‹é¢ä¸¤è¡Œ vvvvvvvvvvvvvv
plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

SEED = 42 # é€‰æ‹©ä¸€ä¸ªä½ å–œæ¬¢çš„æ•°å­—
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# --- Part 1: è¶…å‚æ•°è®¾å®š (æ— å˜åŒ–) ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MEMORY_CAPACITY = 10000
BATCH_SIZE = 128
GAMMA = 0.99
TAU = 0.005
ACTOR_LR = 1e-4
CRITIC_LR = 1e-3
NOISE_STDDEV = 0.1

# --- Part 2: ç»éªŒå›æ”¾æ±  (æ— å˜åŒ–) ---
class ReplayBuffer:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
    def push(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    def sample(self, batch_size):
        states, actions, rewards, next_states, dones = zip(*random.sample(self.memory, batch_size))
        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)
    def __len__(self):
        return len(self.memory)

# --- Part 3 & 4: ç½‘ç»œç»“æ„ (æ— å˜åŒ–) ---
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action):
        super(Actor, self).__init__()
        self.layer1 = nn.Linear(state_dim, 256)
        self.layer2 = nn.Linear(256, 256)
        self.layer3 = nn.Linear(256, action_dim)
        self.max_action = max_action
    def forward(self, state):
        x = F.relu(self.layer1(state))
        x = F.relu(self.layer2(x))
        x = torch.tanh(self.layer3(x)) * self.max_action
        return x

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()
        self.layer1 = nn.Linear(state_dim + action_dim, 256)
        self.layer2 = nn.Linear(256, 256)
        self.layer3 = nn.Linear(256, 1)
    def forward(self, state, action):
        x = torch.cat([state, action], 1)
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        return self.layer3(x)

def calculate_jitter_score(data_series):
    """
    è®¡ç®—ä¸€ä¸ªæ—¶é—´åºåˆ—çš„â€œé¢ ç°¸åº¦â€åˆ†æ•°ã€‚
    åˆ†æ•°è¶Šä½ï¼Œä»£è¡¨æ›²çº¿è¶Šå¹³æ»‘ã€å™ªå£°è¶Šå°‘ã€‚
    """
    # ç¡®ä¿è¾“å…¥æ˜¯ numpy æ•°ç»„
    data_series = np.array(data_series)
    
    # 1. ä½¿ç”¨ np.diff() è®¡ç®—ç›¸é‚»æ•°æ®ç‚¹ä¹‹é—´çš„å·®å€¼
    differences = np.diff(data_series)
    
    # 2. è®¡ç®—è¿™äº›å·®å€¼çš„æ ‡å‡†å·®
    # æ ‡å‡†å·®è¶Šå¤§ï¼Œè¯´æ˜æ•°æ®ç‚¹ä¹‹é—´çš„è·³è·ƒè¶Šå‰§çƒˆã€è¶Šä¸ç¨³å®š
    jitter_score = np.std(differences)
    
    return jitter_score

# --- Part 5: DDPG æ€»æŒ‡æŒ¥ (ä¿®æ”¹trainæ–¹æ³•ä»¥è¿”å›loss) ---
class DDPGAgent:
    def __init__(self, state_dim, action_dim, max_action):
        self.max_action = max_action
        self.actor = Actor(state_dim, action_dim, max_action).to(DEVICE)
        self.actor_target = Actor(state_dim, action_dim, max_action).to(DEVICE)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)
        self.critic = Critic(state_dim, action_dim).to(DEVICE)
        self.critic_target = Critic(state_dim, action_dim).to(DEVICE)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)
        self.memory = ReplayBuffer(MEMORY_CAPACITY)

    def select_action(self, state):
        state = torch.FloatTensor(state.reshape(1, -1)).to(DEVICE)
        action = self.actor(state).cpu().data.numpy().flatten()
        noise = np.random.normal(0, self.max_action * NOISE_STDDEV, size=action.shape)
        return (action + noise).clip(-self.max_action, self.max_action)

    def train(self):
        if len(self.memory) < BATCH_SIZE:
            return None, None # <--- ä¿®æ”¹: å¦‚æœä¸è®­ç»ƒï¼Œè¿”å›None

        states, actions, rewards, next_states, dones = self.memory.sample(BATCH_SIZE)
        states = torch.FloatTensor(states).to(DEVICE)
        actions = torch.FloatTensor(actions).to(DEVICE)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(DEVICE)
        next_states = torch.FloatTensor(next_states).to(DEVICE)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(DEVICE)

        with torch.no_grad():
            target_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, target_actions)
            target_q = rewards + (1 - dones) * GAMMA * target_q
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q)
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        actor_loss = -self.critic(states, self.actor(states)).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)
        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)
            
        return actor_loss.item(), critic_loss.item() # <--- æ–°å¢: è¿”å›losså€¼

    def save_models(self, filename): # <--- æ–°å¢: ä¿å­˜æ¨¡å‹çš„æ–¹æ³•
        torch.save(self.actor.state_dict(), f"{filename}_actor.pth")
        torch.save(self.critic.state_dict(), f"{filename}_critic.pth")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {filename}_actor.pth å’Œ {filename}_critic.pth")


# ==========================================================
# ============ã€å…³é”®ä¿®æ”¹ã€‘æŠŠæ‰€æœ‰æ‰§è¡Œä»£ç æ”¾å…¥è¿™é‡Œ ============
# ==========================================================
if __name__ == '__main__':

    # --- Part 6: ä¸»è®­ç»ƒå¾ªç¯ (ä¿®æ”¹ä»¥è®°å½•å’Œç»˜å›¾) ---
    env = gym.make('Pendulum-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    agent = DDPGAgent(state_dim, action_dim, max_action)
    num_episodes = 200

    # ã€æ–°å¢ã€‘ç”¨äºè¿½è¸ªæœ€ä½³è¡¨ç°çš„å˜é‡ï¼Œåˆå§‹è®¾ä¸ºä¸€ä¸ªæå°çš„å€¼
    best_reward = -float('inf') 

    # <--- æ–°å¢: ç”¨äºè®°å½•æ•°æ®çš„åˆ—è¡¨ ---
    rewards = []
    actor_losses = []
    critic_losses = []

    print("="*30 + "\nå¼€å§‹è®­ç»ƒ...\n" + "="*30)
    for i_episode in range(num_episodes):
        state, info = env.reset(seed=SEED)
        episode_reward = 0
        temp_actor_loss, temp_critic_loss = [], []

        for t in range(200):
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            agent.memory.push(state, action, reward, next_state, done)
            
            # <--- ä¿®æ”¹: æ¥æ”¶è¿”å›çš„losså€¼ ---
            a_loss, c_loss = agent.train()
            if a_loss is not None:
                temp_actor_loss.append(a_loss)
                temp_critic_loss.append(c_loss)

            state = next_state
            episode_reward += reward
            if done:
                break
                
        # <--- æ–°å¢: è®°å½•æ¯ä¸ªå›åˆçš„å¹³å‡losså’Œæ€»reward ---
        rewards.append(episode_reward)
        if temp_actor_loss:
            actor_losses.append(np.mean(temp_actor_loss))
            critic_losses.append(np.mean(temp_critic_loss))
        else: # å¦‚æœç¬¬ä¸€å›åˆæ²¡å¼€å§‹è®­ç»ƒï¼Œå°±è®°ä¸º0
            actor_losses.append(0)
            critic_losses.append(0)

        print(f'\rEpisode: {i_episode+1}/{num_episodes}, Reward: {episode_reward:.2f}', end="")
        if (i_episode + 1) % 20 == 0:
            print("")

        # ==========================================================
        # ============ã€å…³é”®ä¿®æ”¹ã€‘æ¨¡å‹æ£€æŸ¥ç‚¹é€»è¾‘ =====================
        # ==========================================================
        # å¦‚æœå½“å‰å›åˆçš„å¥–åŠ±ï¼Œè¶…è¿‡äº†æˆ‘ä»¬è®°å½•çš„å†å²æœ€ä½³å¥–åŠ±
        if episode_reward > best_reward:
            best_reward = episode_reward # æ›´æ–°æœ€ä½³å¥–åŠ±
            agent.save_models("ddpg_pendulum_best") # ä¿å­˜å½“å‰è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼
            print(f"\nğŸ‰ æ–°çºªå½•ï¼å‘ç°åœ¨ç¬¬ {i_episode+1} å›åˆï¼Œæœ€ä½³å¥–åŠ±: {best_reward:.2f}ã€‚æ¨¡å‹å·²ä¿å­˜ï¼")
        # ==========================================================

    # (å¯é€‰) ä½ ä»ç„¶å¯ä»¥åœ¨æœ€åä¿å­˜ä¸€ä¸ªæœ€ç»ˆæ¨¡å‹ï¼Œç”¨äºå¯¹æ¯”
    agent.save_models("ddpg_pendulum_final")


    # env.close()
    # agent.save_models("ddpg_pendulum") # <--- æ–°å¢: è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹
    print("\nè®­ç»ƒå®Œæˆ!")

    # ... ä½ çš„è®­ç»ƒå¾ªç¯ ...

    #env.close() # <--- æŠŠè¿™è¡Œå…ˆæ³¨é‡Šæ‰æˆ–è€…ç§»åˆ°æœ€å

    # ==========================================================
    # ============ã€æ–°å¢ã€‘åœ¨è®­ç»ƒè„šæœ¬æœ«å°¾è¿›è¡Œå³æ—¶éªŒè¯ ============
    # ==========================================================
    print("\n--- è®­ç»ƒç»“æŸï¼Œå¼€å§‹å³æ—¶éªŒè¯ä¿å­˜çš„æ¨¡å‹ ---")

    # åˆ›å»ºä¸€ä¸ªæ–°çš„ Actor ç½‘ç»œå®ä¾‹
    validation_actor = Actor(state_dim, action_dim, max_action).to(DEVICE)

    # åŠ è½½åˆšåˆšä¿å­˜çš„æƒé‡
    try:
        validation_actor.load_state_dict(torch.load("ddpg_pendulum_best_actor.pth"))
        print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼Œå¼€å§‹è¿›è¡Œå¯è§†åŒ–æµ‹è¯•...")
    except FileNotFoundError:
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ°åˆšåˆšä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ã€‚")
        exit()

    validation_actor.eval()

    # åœ¨ä¸€ä¸ªæ–°çš„ã€å¸¦å›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•
    test_env = gym.make('Pendulum-v1', render_mode='human')
    state, info = test_env.reset(seed=SEED)
    total_reward = 0
    done = False

    try:
        step_count = 0 
        while not done:
            # ã€å…³é”®ä¿®æ”¹ã€‘åœ¨å¾ªç¯çš„å¼€å§‹ï¼Œå¼ºåˆ¶æ¸²æŸ“ä¸€å¸§ç”»é¢
            test_env.render() 
            step_count += 1
            state_tensor = torch.FloatTensor(state.reshape(1, -1)).to(DEVICE)
            with torch.no_grad():
                action = validation_actor(state_tensor).cpu().data.numpy().flatten()
            
            next_state, reward, terminated, truncated, info = test_env.step(action)
            done = terminated or truncated
            
            state = next_state
            total_reward += reward
            time.sleep(0.05) # çŸ­æš‚å»¶æ—¶æ–¹ä¾¿è§‚å¯Ÿ
        print(f"éªŒè¯å¾ªç¯æ€»å…±è¿è¡Œäº† {step_count} æ­¥ã€‚")

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­éªŒè¯ã€‚")

    print(f"\nå³æ—¶éªŒè¯ç»“æŸã€‚æœ€ç»ˆå¥–åŠ±: {total_reward:.2f}")
    test_env.close()
    env.close() # <--- ç§»åˆ°è¿™é‡Œ

    # <--- æ–°å¢: ç»˜åˆ¶å¤šä¸ªå›¾è¡¨ ---
    fig, axs = plt.subplots(3, 1, figsize=(10, 15))
    fig.suptitle('DDPG è®­ç»ƒè¿‡ç¨‹åˆ†æ')

    # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
    axs[0].plot(rewards)
    axs[0].set_title('æ¯å›åˆå¥–åŠ± (Reward)')
    axs[0].set_xlabel('å›åˆ (Episode)')
    axs[0].set_ylabel('æ€»å¥–åŠ± (Total Reward)')
    jitter_score_reward = calculate_jitter_score(rewards)
    print(f"ddpg reward jitter: {jitter_score_reward:3f}")

    # ç»˜åˆ¶Actor Lossæ›²çº¿
    axs[1].plot(actor_losses)
    axs[1].set_title('æ¼”å‘˜ç½‘ç»œæŸå¤± (Actor Loss)')
    axs[1].set_xlabel('å›åˆ (Episode)')
    axs[1].set_ylabel('æŸå¤± (Loss)')
    jitter_score_act = calculate_jitter_score(actor_losses)
    print(f"ddpg actor jitter: {jitter_score_act:3f}")

    # ç»˜åˆ¶Critic Lossæ›²çº¿
    axs[2].plot(critic_losses)
    axs[2].set_title('è¯„è®ºå®¶ç½‘ç»œæŸå¤± (Critic Loss)')
    axs[2].set_xlabel('å›åˆ (Episode)')
    axs[2].set_ylabel('æŸå¤± (Loss)')
    jitter_score_critic = calculate_jitter_score(critic_losses)
    print(f"ddpg critic jitter: {jitter_score_critic:3f}")

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()