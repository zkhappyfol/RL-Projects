import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import namedtuple, deque
from itertools import count
import cv2
import matplotlib.pyplot as plt
import time
import math
import ale_py

# --- Part 1: è¶…å‚æ•° ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 32
GAMMA = 0.99
# Epsilon-Greedy å‚æ•°
EPS_START = 1.0
EPS_END = 0.01
EPS_DECAY = 30000
# è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°
TAU = 0.005
LR = 1e-4
MEMORY_SIZE = 100000 # è®°å¿†å®«æ®¿å®¹é‡
NUM_EPISODES = 1000 # è®­ç»ƒå›åˆæ•°

# --- Part 2: é¢„å¤„ç†åŒ…è£…å™¨ (æˆ‘ä»¬ä¹‹å‰å†™å¥½çš„) ---
class PreprocessFrame(gym.ObservationWrapper):
    def __init__(self, env, width, height):
        super().__init__(env)
        self.width = width
        self.height = height
        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8
        )
    def observation(self, obs):
        gray_obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
        resized_obs = cv2.resize(gray_obs, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return resized_obs[:, :, None]

class StackFrames(gym.ObservationWrapper):
    def __init__(self, env, k):
        super().__init__(env)
        self.k = k
        self.frames = deque([], maxlen=k)
        height, width, _ = env.observation_space.shape
        self.observation_space = gym.spaces.Box(
            low=0, high=255, shape=(k, height, width), dtype=np.uint8 # <--- æ³¨æ„: æ”¹ä¸ºé€šé“åœ¨å‰
        )
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        for _ in range(self.k):
            self.frames.append(obs)
        return self._get_obs(), info
    def observation(self, obs):
        self.frames.append(obs)
        return self._get_obs()
    def _get_obs(self):
        # å°†dequeä¸­çš„kä¸ªå¸§åœ¨é€šé“ç»´åº¦ä¸Šæ‹¼æ¥
        frames_np = np.concatenate(list(self.frames), axis=2)
        # ä» (H, W, C) è½¬æ¢åˆ° (C, H, W)
        return frames_np.transpose(2, 0, 1)

# --- Part 3: CNN å¤§è„‘ (æˆ‘ä»¬ä¹‹å‰å†™å¥½çš„) ---
class CnnDQN(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(CnnDQN, self).__init__()
        self.input_shape = input_shape
        self.num_actions = num_actions
        self.conv_layers = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()
        )
        dummy_input = torch.zeros(1, *input_shape)
        conv_out_size = self._get_conv_out(dummy_input)
        self.fc_layers = nn.Sequential(
            nn.Linear(conv_out_size, 512), nn.ReLU(),
            nn.Linear(512, num_actions)
        )
    def _get_conv_out(self, x):
        o = self.conv_layers(x)
        return int(torch.flatten(o, 1).size(1))
    def forward(self, x):
        # è¾“å…¥å›¾åƒçš„åƒç´ å€¼èŒƒå›´æ˜¯[0, 255]ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶å½’ä¸€åŒ–åˆ°[0, 1]
        x = x.float() / 255.0
        conv_out = self.conv_layers(x)
        flattened = torch.flatten(conv_out, 1)
        q_values = self.fc_layers(flattened)
        return q_values

# --- Part 4: è®°å¿†å®«æ®¿ (è€æœ‹å‹) ---
Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))
class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)
    def push(self, *args):
        self.memory.append(Experience(*args))
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    def __len__(self):
        return len(self.memory)

# --- Part 5: ä¸»è®­ç»ƒé€»è¾‘ ---
def main():
    # 1. åˆ›å»ºå¹¶å°è£…ç¯å¢ƒ
    env = gym.make("ALE/Pong-v5")
    env = PreprocessFrame(env, width=84, height=84)
    env = StackFrames(env, k=4)

    n_actions = env.action_space.n
    state, info = env.reset()
    n_observations = state.shape

    # 2. åˆ›å»ºç½‘ç»œã€ä¼˜åŒ–å™¨ã€è®°å¿†å®«æ®¿
    policy_net = CnnDQN(n_observations, n_actions).to(DEVICE)
    target_net = CnnDQN(n_observations, n_actions).to(DEVICE)
    target_net.load_state_dict(policy_net.state_dict())
    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
    memory = ReplayMemory(MEMORY_SIZE)
    
    steps_done = 0
    episode_rewards = []

    highest_score = -float('inf') # <--- æ–°å¢ï¼šåˆå§‹åŒ–æœ€é«˜åˆ†è®°å½•ä¸ºè´Ÿæ— ç©·å¤§

    print("="*40)
    print(f"å¼€å§‹åœ¨ {DEVICE} ä¸Šè®­ç»ƒ Pong...")
    print(f"æ€»å…±è¿›è¡Œ {NUM_EPISODES} ä¸ªå›åˆçš„è®­ç»ƒã€‚")
    print("="*40)
    
    start_time = time.time() # <--- æ–°å¢: è®°å½•å¼€å§‹æ—¶é—´

    for i_episode in range(NUM_EPISODES):
        state, info = env.reset()
        state = torch.tensor(np.array(state), dtype=torch.uint8, device=DEVICE).unsqueeze(0)
        
        episode_reward = 0
        for t in count():
            # 3. é€‰æ‹©åŠ¨ä½œ (Epsilon-Greedy)
            eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
            steps_done += 1
            if random.random() > eps_threshold:
                with torch.no_grad():
                    action = policy_net(state).max(1)[1].view(1, 1)
            else:
                action = torch.tensor([[env.action_space.sample()]], device=DEVICE, dtype=torch.long)

            # <--- æ–°å¢: â€œå¿ƒè·³â€æ—¥å¿— --- >
            if steps_done % 1000 == 0:
                elapsed_time = time.time() - start_time
                print(f"  ... æ€»æ­¥æ•°: {steps_done}, Epsilon: {eps_threshold:.4f}, å·²ç”¨æ—¶: {elapsed_time:.2f}s")


            # 4. ä¸ç¯å¢ƒäº’åŠ¨
            observation, reward, terminated, truncated, _ = env.step(action.item())
            episode_reward += reward
            reward = torch.tensor([reward], device=DEVICE)
            done = terminated or truncated

            if done:
                next_state = None
            else:
                next_state = torch.tensor(np.array(observation), dtype=torch.uint8, device=DEVICE).unsqueeze(0)

            # 5. å­˜å…¥è®°å¿†
            memory.push(state, action, reward, next_state, done)
            state = next_state

            # 6. å­¦ä¹ ï¼(æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–)
            if len(memory) > BATCH_SIZE:
                experiences = memory.sample(BATCH_SIZE)
                batch = Experience(*zip(*experiences))
                
                # ... (çœç•¥äº†å¤„ç† non_final_next_states çš„ä»£ç ï¼Œè¿™éƒ¨åˆ†ä¸æˆ‘ä»¬TD3ä¸­çš„ä¿®å¤ç±»ä¼¼) ...
                # ä¸ºäº†ç®€æ´ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨PyTorchå®˜æ–¹æ•™ç¨‹çš„ç®€åŒ–å†™æ³•
                
                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=DEVICE, dtype=torch.bool)
                non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
                
                state_batch = torch.cat(batch.state)
                action_batch = torch.cat(batch.action)
                reward_batch = torch.cat(batch.reward)

                predicted_q_values = policy_net(state_batch).gather(1, action_batch)
                
                next_state_values = torch.zeros(BATCH_SIZE, device=DEVICE)
                with torch.no_grad():
                    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]
                
                target_q_values = (next_state_values * GAMMA) + reward_batch

                criterion = nn.SmoothL1Loss()
                loss = criterion(predicted_q_values, target_q_values.unsqueeze(1))
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100) # æ¢¯åº¦è£å‰ª
                optimizer.step()

            # 7. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            target_net_state_dict = target_net.state_dict()
            policy_net_state_dict = policy_net.state_dict()
            for key in policy_net_state_dict:
                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)
            target_net.load_state_dict(target_net_state_dict)

            if done:
                episode_rewards.append(episode_reward)
                # <--- ä¿®æ”¹: è®©å›åˆç»“æŸçš„æ—¥å¿—æ›´è¯¦ç»† --- >
                avg_reward_last_100 = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
                print(f"å›åˆ {i_episode+1} ç»“æŸ | æ­¥æ•°: {t+1} | å¾—åˆ†: {episode_reward} | æœ€è¿‘100å›åˆå¹³å‡åˆ†: {avg_reward_last_100:.2f}")
                
                # <--- æ–°å¢çš„â€œé‡Œç¨‹ç¢‘æ—¥å¿—â€ --->
                if episode_reward > highest_score:
                    highest_score = episode_reward
                    # æ‰“å°ä¸€æ¡å¸¦ ğŸ‰ åº†ç¥è¡¨æƒ…çš„ã€ç‰¹åˆ«æ˜¾çœ¼çš„æ–°çºªå½•ä¿¡æ¯ï¼
                    print(f"ğŸ‰ æ–°çºªå½•è¯ç”Ÿï¼ Highest Score: {highest_score:.2f}")
                # <--- æ–°å¢ç»“æŸ --->

                break
    
    print("è®­ç»ƒå®Œæˆ!")

    # <--- åœ¨è¿™é‡Œæ·»åŠ ä¸‹é¢çš„ä»£ç  --->
    # 1. å®šä¹‰ä¿å­˜è·¯å¾„
    MODEL_SAVE_PATH = "dqn_pong_model.pth"
    # 2. ä¿å­˜ policy_net çš„â€œå¤§è„‘â€å‚æ•°
    torch.save(policy_net.state_dict(), MODEL_SAVE_PATH)
    print(f"æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    # <--- æ·»åŠ ç»“æŸ --->

    # ç»˜å›¾
    plt.plot(episode_rewards)
    plt.title('DQN on Pong Training')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.show()

# --- Part 6: å¯åŠ¨ï¼ ---
if __name__ == '__main__':
    main()